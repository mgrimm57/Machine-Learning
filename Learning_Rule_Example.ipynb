{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErNL0aPEnHEH"
      },
      "source": [
        "# Exercise 1: A perceptron learning rule\n",
        "\n",
        "Consider a perceptron with activity rule ${g}(h)={1\\over 1+e^{-h}}$, where $h=\\sum_{i=1}^N w_ix_i$ is the activation for an N-dimensional input pattern $x_i$. The training set includes $p$ patterns $\\{x_i^\\mu,\\zeta^\\mu\\}_{\\mu=1,\\ldots,p}$, where $\\zeta^\\mu$ are the targets.\n",
        "\n",
        "**Question 1**: Derive the perceptron learning rule starting from the following cost function:\n",
        "\\begin{align}\n",
        "E={1\\over 2}\\sum_{\\mu=1}^p\\left[\\zeta^\\mu-{g}(h^\\mu)\\right]^2 \\ ,\\label{cost}\\tag{1}\n",
        "\\end{align}\n",
        "by applying gradient descent. Note that $\\partial_h{g}={g}(1-{g})$.\n",
        "\n",
        "**Question 2**: Add a weight decay term to the cost function and derive the new perceptron learning rule. Explain in words what the Bayesian interpretation of the weight decay term is, and what issue with training it may alleviate.\n",
        "\n",
        "**Question 3**: What is the learning rule if you use a linear activity rule ${g}(h)=h$ instead of the sigmoid activity rule as above?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mscjgHUHnHEI"
      },
      "source": [
        "### Question 1\n",
        "First we will differentiate the cost function with respect to the weights. By applying the chain rule we get the following:\n",
        "\\begin{align*}\n",
        "-\\frac{\\partial E}{\\partial w_i}&=-2*\\frac{1}{2}\\sum_{\\mu=1}^p\\left[\\zeta^\\mu-{g}(h^\\mu)\\right]g'(h^\\mu)\\\\\n",
        "\\end{align*}\n",
        "Note that by the chain rule, $g'(h^\\mu)= (\\partial_hg)(\\partial_wg)$. Therefore,\n",
        "\\begin{align*}\n",
        "-\\frac{\\partial E}{\\partial w_i}&=-\\sum_{\\mu=1}^p\\left[\\zeta^\\mu-{g}(h^\\mu)\\right](\\partial_hg)(\\partial_wg)\\\\\n",
        "&=-\\sum_{\\mu=1}^p\\left[\\zeta^\\mu-{g}(h^\\mu)\\right](g(1-g))(x_i^\\mu)\\\\\n",
        "\\end{align*}\n",
        "Therefore our perceptron learning rule would be the following.\n",
        "\\begin{align*}\n",
        "\\delta^\\mu&=g'(h^\\mu)\\left(\\zeta^\\mu-{g}(h^\\mu)\\right)\\\\\n",
        "\\text{or}\\\\\n",
        "\\delta^\\mu&=\\left(\\zeta^\\mu-{g}(h^\\mu)\\right)(g(1-g))(x_i^\\mu)\n",
        "\\end{align*}\n",
        "Finally, to update our weight we would follow this formula below.\n",
        "\\begin{align*}\n",
        "w_i^{n+1}=w_i^{n}+\\eta*\\left(\\zeta^\\mu-{g}(h^\\mu)\\right)(g(1-g))(x_i^\\mu)\n",
        "\\end{align*}\n",
        "where $\\eta$ is our learning rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQnFfehHnHEI"
      },
      "source": [
        "### Question 2\n",
        "New weight decay term: $D(w)=\\alpha\\frac{1}{2}\\sum_i^N w_{i=1}^2$ where $\\alpha$ is a constant we can change to alter the weight of this decay term (weight decay rate). Our updated cost function takes the form\n",
        "\\begin{align*}\n",
        "E_{decay}&=\\frac{1}{2}\\sum_{\\mu=1}^p\\left[\\zeta^\\mu-{g}(h^\\mu)\\right]^2+\\alpha\\frac{1}{2}\\sum_{i=1}^N w_i^2.\\\\\n",
        "\\end{align*}\n",
        "Similiar to Question 1, we will apply gradient descent to find our new learning rule. Hence,\n",
        "\\begin{align*}\n",
        "-\\frac{\\partial E_{decay}}{\\partial w_i} &= -\\sum_{\\mu=1}^p\\left[\\zeta^\\mu-{g}(h^\\mu)\\right]g'(h^\\mu) - \\alpha x_i\n",
        "\\end{align*}\n",
        "Finally, to update our weight we would follow this formula below.\n",
        "\\begin{align*}\n",
        "w_i^{n+1}=w_i^{n}+\\eta*\\left(\\zeta^\\mu-{g}(h^\\mu)\\right)(g(1-g))(x_i^\\mu) -\\alpha x_i\n",
        "\\end{align*}\n",
        "where $\\eta$ is our learning rate.\n",
        "\n",
        "#### Bayesian interpretation of this weight decay term:\n",
        "This weight decay term serves as the prior on the weights. The prior is important to include when we are learning because it takes into account the uncertainty on the parameters estimation.\n",
        "\n",
        "Without the weight decay term, our weights will continue to grow and grow the more we train. This will cause our predictions to be overconfident in the classifications (overfitting). When we include the weight decay term, we hope to alleviate this problem of overfitting our data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QoUPkEwnHEI"
      },
      "source": [
        "### Question 3\n",
        "Here, we will differentiate our cost function (similiar to #1) but now with our updated activity rule. Hence,\n",
        "\\begin{align*}\n",
        "-\\frac{\\partial E}{\\partial w_i}&=-2*\\frac{1}{2}\\sum_{\\mu=1}^p\\left[\\zeta^\\mu-{g}(h^\\mu)\\right]g'(h^\\mu)\\\\\n",
        "&=-\\sum_{\\mu=1}^p\\left[\\zeta^\\mu-{g}(h^\\mu)\\right](\\partial_hg)(\\partial_wg)\\\\\n",
        "&=-\\sum_{\\mu=1}^p\\left[\\zeta^\\mu-{g}(h^\\mu)\\right](1)(x_i^\\mu)\\\\\n",
        "&=-\\sum_{\\mu=1}^p\\left[\\zeta^\\mu-{g}(h^\\mu)\\right](x_i^\\mu)\\\\\n",
        "\\end{align*}\n",
        "Therefore our perceptron learning rule would be the following.\n",
        "\\begin{align*}\n",
        "\\delta^\\mu=\\left(\\zeta^\\mu-{g}(h^\\mu)\\right)(x_i^\\mu)\\\\\n",
        "\\end{align*}\n",
        "Finally, to update our weight we would follow this formula below.\n",
        "\\begin{align*}\n",
        "w_i^{n+1}=w_i^{n}+\\eta*\\left(\\zeta^\\mu-{g}(h^\\mu)\\right)(x_i^\\mu)\n",
        "\\end{align*}\n",
        "where $\\eta$ is our learning rate."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}